# app.py â€” Ultimate Asset Risk, Volatility & Pair Dashboard (2 assets)
# Un seul fichier â€” prÃªt pour Streamlit Cloud/Replit.

import streamlit as st
import pandas as pd
import numpy as np
import yfinance as yf
import plotly.express as px
import plotly.graph_objects as go
import statsmodels.api as sm
from statsmodels.tsa.stattools import coint, adfuller, grangercausalitytests
from statsmodels.stats.diagnostic import acorr_ljungbox, het_arch
from statsmodels.stats.stattools import jarque_bera
from statsmodels.tsa.regime_switching.markov_regression import MarkovRegression
from statsmodels.regression.quantile_regression import QuantReg
from scipy.stats import norm, genpareto
from datetime import date, timedelta

# ------------- CONFIG -------------
st.set_page_config(page_title="Ultimate Asset Risk Dashboard", page_icon="ðŸ“ˆ", layout="wide")
st.title("ðŸ“Š Ultimate Asset Risk, Volatility & Pair Dashboard â€” 2 actifs")

FREQ_MAP = {
    "Journalier (1d)": ("1d", 252, "D"),      # interval_yf, annualization, pandas rule (after resample)
    "Hebdo (1wk)":     ("1wk", 52,  "W-FRI"),
    "Mensuel (1mo)":   ("1mo", 12,  "M"),
}
DEFAULT_FREQ = "Journalier (1d)"

# ------------- HELPERS -------------
def ann_factor(freq_key: str) -> int:
    return FREQ_MAP[freq_key][1]

def resample_rule(freq_key: str) -> str:
    return FREQ_MAP[freq_key][2]

def range_kwargs(start_date, end_date):
    # uniformiser lâ€™abscisse sur toute lâ€™app
    return dict(range_x=[pd.Timestamp(start_date), pd.Timestamp(end_date)])

def pct(x, d=2):
    return "â€”" if x is None or pd.isna(x) else f"{x*100:.{d}f}%"

def safe_two_cols(df: pd.DataFrame, a: str, b: str) -> pd.DataFrame:
    cols = [c for c in df.columns if str(c) in [a, b]]
    return df[cols].copy()

def strict_resample(prices_daily: pd.DataFrame, rule: str) -> pd.DataFrame:
    """Resample en fin de pÃ©riode (last) pour stabiliser la lecture finance."""
    if rule == "D":  # dÃ©jÃ  journalier
        return prices_daily.copy()
    return prices_daily.resample(rule).last()

def returns(prices: pd.DataFrame, log=False) -> pd.DataFrame:
    return (np.log(prices/prices.shift(1)) if log else prices.pct_change()).dropna()

def drawdown_series(prices: pd.Series) -> pd.Series:
    return prices/prices.cummax() - 1.0

def max_drawdown(prices: pd.Series) -> float:
    return float(drawdown_series(prices).min())

def ann_vol(ret: pd.Series, k: int) -> float:
    return float(ret.std(ddof=1)*np.sqrt(k))

def sharpe(ret: pd.Series, k: int, rf_annual: float) -> float:
    if ret.dropna().empty: return np.nan
    rf_p = (1+rf_annual)**(1/k) - 1
    sig = ret.std(ddof=1)
    return float(((ret.mean()-rf_p)/sig)*np.sqrt(k)) if sig>0 else np.nan

def sortino(ret: pd.Series, k: int, rf_annual: float) -> float:
    rf_p = (1+rf_annual)**(1/k) - 1
    dn = ret[ret<0]
    if dn.dropna().empty: return np.nan
    sigd = dn.std(ddof=1)
    return float(((ret.mean()-rf_p)/sigd)*np.sqrt(k)) if sigd>0 else np.nan

def cornish_fisher_var(ret: pd.Series, q=0.95):
    x = ret.dropna()
    if x.empty: return np.nan
    mu, sig = x.mean(), x.std(ddof=1)
    S, K = x.skew(), x.kurt()  # excess kurt
    z = norm.ppf(1-q)
    zcf = z + (1/6)*(z**2-1)*S + (1/24)*(z**3-3*z)*K - (1/36)*(2*z**3-5*z)*S**2
    return float(-(mu + zcf*sig))

def hist_var_es(ret: pd.Series, q=0.95):
    r = ret.dropna().values
    if r.size==0: return np.nan, np.nan
    qv = np.quantile(r, 1-q)
    var_loss = -float(qv)
    es_loss = -float(r[r<=qv].mean()) if (r<=qv).any() else np.nan
    return var_loss, es_loss

def beta_alpha_ols(x: pd.Series, y: pd.Series):
    df = pd.concat([x,y], axis=1).dropna()
    if df.empty: return np.nan, np.nan, np.nan
    X = sm.add_constant(df.iloc[:,0])
    m = sm.OLS(df.iloc[:,1], X).fit()
    return float(m.params[1]), float(m.params[0]), float(m.rsquared)

def rolling_beta(x: pd.Series, y: pd.Series, w: int) -> pd.Series:
    xy = pd.concat([x,y], axis=1).dropna()
    cov = xy[x.name].rolling(w).cov(xy[y.name])
    var = xy[x.name].rolling(w).var()
    return (cov/var).rename("Î² glissant")

def ou_half_life(spread: pd.Series) -> float:
    x = (spread - spread.mean()).dropna()
    if len(x)<30: return np.nan
    x1 = x.shift(1).dropna()
    xt = x.loc[x1.index]
    rho, *_ = np.linalg.lstsq(x1.values.reshape(-1,1), xt.values, rcond=None)
    rho = float(rho[0])
    if rho<=0 or rho>=1: return np.nan
    return float(-np.log(2)/np.log(rho))

def ewma_mu_cov(rets: pd.DataFrame, lam: float):
    r = rets.dropna()
    if r.empty: return r.mean(), r.cov()
    w = np.array([lam**k for k in range(len(r))])[::-1]; w /= w.sum()
    mu = pd.Series((r.values*w[:,None]).sum(axis=0), index=r.columns)
    S = np.zeros((r.shape[1], r.shape[1]))
    m = r - mu
    for t in range(len(m)):
        v = m.iloc[t].values.reshape(-1,1)
        S = lam*S + (1-lam)*(v@v.T)
    return mu, pd.DataFrame(S, index=r.columns, columns=r.columns)

def min_var_w(cov: pd.DataFrame) -> pd.Series:
    inv = np.linalg.pinv(cov.values)
    ones = np.ones((cov.shape[0],1))
    w = inv@ones; w = w/(ones.T@inv@ones)
    return pd.Series(w.flatten(), index=cov.columns)

def tangency_w(mu: pd.Series, cov: pd.DataFrame, rf: float) -> pd.Series:
    mu_ex = mu.values - rf
    inv = np.linalg.pinv(cov.values)
    w = inv@mu_ex
    w = w/(np.ones(len(mu))@w)
    return pd.Series(w, index=mu.index)

def port_stats(w: pd.Series, mu: pd.Series, cov: pd.DataFrame, rf: float):
    mu_p = float(w.values@mu.values)
    sig_p = float(np.sqrt(w.values@cov.values@w.values))
    S = (mu_p-rf)/sig_p if sig_p>0 else np.nan
    return mu_p, sig_p, S

def tail_corr(a: pd.Series, b: pd.Series, q=0.05, upper=False):
    """CorrÃ©lation conditionnelle dans la queue : les deux < q (ou > 1-q)."""
    if upper:
        mask = (a >= a.quantile(1-q)) & (b >= b.quantile(1-q))
    else:
        mask = (a <= a.quantile(q)) & (b <= b.quantile(q))
    sub = pd.concat([a[mask], b[mask]], axis=1).dropna()
    return float(sub.corr().iloc[0,1]) if len(sub)>5 else np.nan

def xcorr_at_lags(a: pd.Series, b: pd.Series, lags=5):
    """Cross-corr lead/lag : corr(b_t, a_{t-k}) et corr(a_t, b_{t-k}) pour k=1..lags."""
    res = []
    for k in range(1, lags+1):
        res.append({
            "lag": k,
            "corr Aâ†’B": pd.concat([b, a.shift(k)], axis=1).dropna().corr().iloc[0,1],
            "corr Bâ†’A": pd.concat([a, b.shift(k)], axis=1).dropna().corr().iloc[0,1],
        })
    return pd.DataFrame(res)

# ------------- SIDEBAR -------------
with st.sidebar:
    st.header("âš™ï¸ ParamÃ¨tres")
    c = st.columns(2)
    with c[0]: ticker_a = st.text_input("Ticker A", "AAPL")
    with c[1]: ticker_b = st.text_input("Ticker B", "MSFT")

    freq_key = st.selectbox("FrÃ©quence", list(FREQ_MAP.keys()), index=list(FREQ_MAP.keys()).index(DEFAULT_FREQ))
    k = ann_factor(freq_key)
    rule = resample_rule(freq_key)

    today = date.today()
    d = st.columns(2)
    with d[0]: start_date = st.date_input("Date dÃ©but", value=today - timedelta(days=365*3))
    with d[1]: end_date   = st.date_input("Date fin", value=today)

    log_ret = st.checkbox("Rendements logarithmiques", False)
    rf_annual = st.number_input("Taux sans risque annualisÃ© (%)", 0.0, step=0.25)/100.0

    roll_vol_w  = st.slider(f"FenÃªtre vol ({freq_key})", 10, 200, 30, step=5)
    roll_corr_w = st.slider(f"FenÃªtre corr ({freq_key})", 10, 260, 60, step=5)
    roll_beta_w = st.slider(f"FenÃªtre Î² ({freq_key})",   10, 260, 60, step=5)

    with st.expander("ParamÃ¨tres avancÃ©s"):
        lam = st.slider("Î» EWMA (RiskMetrics)", 0.80, 0.99, 0.94, 0.01)
        long_only = st.checkbox("Contraindre portefeuilles long-only", True)
        enable_heavy = st.checkbox("Activer modÃ¨les lourds (GARCH, rÃ©gimes)", False)
        lags_xcorr = st.slider("Lags cross-corr (1..20)", 3, 20, 5)

    st.caption("âœ… Tous les graphiques sont cadrÃ©s sur la plage de dates sÃ©lectionnÃ©e. Les donnÃ©es sont **rÃ©cupÃ©rÃ©es en journalier** sur la plage (avec buffer technique), puis **resamplÃ©es** selon la frÃ©quence choisie.")

# ------------- DATA FETCH (STRICT DATE HANDLING) -------------
@st.cache_data(show_spinner=True)
def fetch_daily_close(tickers, start, end):
    """TÃ©lÃ©charge en journalier pour couvrir toute la plage. Retour: Close columns."""
    df = yf.download(tickers=tickers, start=start, end=end, interval="1d",
                     auto_adjust=True, progress=False, group_by="ticker", threads=True)
    if df is None or df.empty:
        return None
    if isinstance(df.columns, pd.MultiIndex):
        try:
            df = df.loc[:, pd.IndexSlice[:, "Close"]]
            df.columns = [c[0] for c in df.columns]
        except Exception:
            if "Close" in df.columns:
                df = df["Close"]
    elif "Close" in df.columns:
        df = df["Close"]
    return df.sort_index()

# buffer pour fiabiliser les fenÃªtres glissantes et resampling
max_win = max(roll_vol_w, roll_corr_w, roll_beta_w)
pre_buffer_days = int(max_win*3)  # buffer gÃ©nÃ©reux
fetch_start = pd.Timestamp(start_date) - pd.Timedelta(days=pre_buffer_days)
fetch_end   = pd.Timestamp(end_date)   + pd.Timedelta(days=2)  # +2 pour inclure la derniÃ¨re clÃ´ture

raw_daily = fetch_daily_close([ticker_a, ticker_b], str(fetch_start.date()), str(fetch_end.date()))
if raw_daily is None or raw_daily.empty:
    st.warning("DonnÃ©es indisponibles (Yahoo Finance). VÃ©rifie tickers/pÃ©riode.")
    st.stop()

# VÃ©rifier que les 2 colonnes existent
missing_cols = [t for t in [ticker_a, ticker_b] if t not in raw_daily.columns]
if missing_cols:
    st.error(f"Les tickers suivants n'ont pas Ã©tÃ© trouvÃ©s dans les donnÃ©es: {missing_cols}. "
             f"Colonnes reÃ§ues: {list(raw_daily.columns)}")
    st.stop()

# Alignement, ffill et strict filter
raw_daily = safe_two_cols(raw_daily.ffill(), ticker_a, ticker_b).dropna(how="all")
prices_resampled = strict_resample(raw_daily, rule).ffill()
prices = prices_resampled.loc[(prices_resampled.index>=pd.Timestamp(start_date)) &
                              (prices_resampled.index<=pd.Timestamp(end_date))].copy()

if prices.empty:
    st.warning("Aucune donnÃ©e aprÃ¨s resampling/filtre strict. Ã‰largis la pÃ©riode ou change la frÃ©quence.")
    st.stop()

rets = returns(prices, log_ret)
if rets.empty:
    st.warning("Rendements insuffisants aprÃ¨s nettoyage.")
    st.stop()

# Audit de complÃ©tude : couverture attendue vs observÃ©e (utilise jours ouvrÃ©s pour le journalier)
def expected_points(start, end, rule):
    freq = "B" if rule == "D" else rule  # "B" = business days
    rng = pd.date_range(start=start, end=end, freq=freq)
    return len(rng)

expected = expected_points(start_date, end_date, rule)
coverage = len(prices)/expected if expected>0 else np.nan
st.sidebar.markdown(f"**Audit data** : points observÃ©s = {len(prices)} / attendus â‰ˆ {expected} â†’ couverture â‰ˆ **{coverage:.0%}**")
if coverage < 0.7:
    st.sidebar.warning("Couverture < 70% : possible manque de data (jours fÃ©riÃ©s/actif rÃ©cent). InterprÃ©tez avec prudence.")

# ------------- TABS -------------
tab_over, tab_roll, tab_port, tab_pair, tab_risk, tab_diag, tab_dep, tab_data = st.tabs(
    ["Overview", "Rolling", "Portfolio", "Pair Trading", "Advanced Risk", "Diagnostics", "DÃ©pendances de queue & Lags", "Data"]
)

# ========== OVERVIEW ==========
with tab_over:
    corr = rets[[ticker_a, ticker_b]].corr().iloc[0,1]
    beta, alpha, r2 = beta_alpha_ols(rets[ticker_a], rets[ticker_b])
    try:
        coint_p = float(coint(prices[ticker_a], prices[ticker_b])[1])
    except Exception:
        coint_p = np.nan

    kpi = st.columns(5)
    kpi[0].metric("CorrÃ©lation", f"{corr:.3f}")
    kpi[1].metric("Î² (B~A)", f"{beta:.3f}")
    kpi[2].metric("Î± (par pÃ©riode)", f"{alpha:.5f}")
    kpi[3].metric("RÂ² OLS", f"{r2:.3f}")
    kpi[4].metric("p-val cointÃ©gration", f"{coint_p:.3f}")

    st.caption("**InterprÃ©tation** â€” CorrÃ©lation: comouvement global; Î²: sensibilitÃ© de B aux variations de A; Î±: performance idiosyncratique; RÂ²: qualitÃ© dâ€™ajustement; cointÃ©gration: relation dâ€™Ã©quilibre de long terme (p<0,05).")

    # Table indicateurs
    def metrics_block(P: pd.Series, R: pd.Series):
        var95, es95 = hist_var_es(R, 0.95)
        return {
            "Dernier prix": float(P.iloc[-1]),
            "CAGR": (P.iloc[-1]/P.iloc[0])**(k/len(P)) - 1 if len(P)>=2 else np.nan,
            "Vol (ann.)": ann_vol(R, k),
            "Sharpe": sharpe(R, k, rf_annual),
            "Sortino": sortino(R, k, rf_annual),
            "Max DD": max_drawdown(P),
            "VaR95 (hist/period)": var95,
            "ES95 (hist/period)": es95,
            "VaR95 (Cornish-Fisher)": cornish_fisher_var(R),
            "Skew": float(R.skew()),
            "Kurtosis (excess)": float(R.kurt()),
        }
    mA = metrics_block(prices[ticker_a], rets[ticker_a])
    mB = metrics_block(prices[ticker_b], rets[ticker_b])
    tbl = pd.DataFrame([mA, mB], index=[ticker_a, ticker_b]).copy()
    for col in ["CAGR","Vol (ann.)","Max DD","VaR95 (hist/period)","ES95 (hist/period)","VaR95 (Cornish-Fisher)"]:
        tbl[col] = tbl[col].apply(pct)
    st.subheader("Indicateurs clÃ©s (annualisÃ©s)")
    st.dataframe(tbl, use_container_width=True)
    st.caption("**Mode dâ€™emploi** â€” Utilise CAGR/Sharpe/Sortino pour classer le couple rendement/risque; Max DD pour le risque extrÃªme; VaR/ES pour pertes attendues en queue; Skew/Kurtosis pour la forme de distribution (asymÃ©trie, queues Ã©paisses).")

    # Prix indexÃ©s
    st.subheader("Prix indexÃ©s (base=100)")
    idx = prices/prices.iloc[0]*100
    fig_idx = px.line(idx, x=idx.index, y=idx.columns, labels={"value":"Indice","variable":"Actif","index":"Date"}, **range_kwargs(start_date,end_date))
    st.plotly_chart(fig_idx, use_container_width=True, theme="streamlit")
    st.caption("**Pourquoi** â€” Met les actifs sur une Ã©chelle comparable; **Comment** â€” Regarde qui surperforme (sâ€™Ã©loigne >100) sur la fenÃªtre choisie.")

    # Scatter + droite OLS (âœ… correctifs)
    st.subheader("Dispersion des rendements (B vs A) + OLS")
    XY = rets[[ticker_a, ticker_b]].dropna().copy()
    X = sm.add_constant(XY[ticker_a])
    y = XY[ticker_b]
    mod = sm.OLS(y, X).fit()
    XY["pred"] = mod.predict(X)
    fig_sc = px.scatter(XY, x=ticker_a, y=ticker_b, opacity=0.6)
    fig_sc.add_traces(px.line(XY.sort_values(ticker_a), x=ticker_a, y="pred").data)
    st.plotly_chart(fig_sc, use_container_width=True, theme="streamlit")
    st.caption("**Lecture** â€” La pente correspond Ã  Î²; les points loin de la droite sont des anomalies/idiosyncratiques; utile pour **hedging** et **pairs**.")

    # Drawdowns
    st.subheader("Drawdowns cumulÃ©s")
    dd = pd.concat([drawdown_series(prices[ticker_a]).rename(ticker_a),
                    drawdown_series(prices[ticker_b]).rename(ticker_b)], axis=1)
    fig_dd = px.line(dd, x=dd.index, y=dd.columns, labels={"value":"Drawdown","variable":"Actif","index":"Date"}, **range_kwargs(start_date,end_date))
    st.plotly_chart(fig_dd, use_container_width=True, theme="streamlit")
    st.caption("**Ã€ quoi Ã§a sert** â€” Visualise les pertes par rapport au pic; utile pour jauger lâ€™**expÃ©rience investisseur** en pÃ©riode difficile.")

# ========== ROLLING ==========
with tab_roll:
    st.subheader("VolatilitÃ© glissante (annualisÃ©e)")
    rv = rets.rolling(roll_vol_w).std(ddof=1)*np.sqrt(k)
    fig_rv = px.line(rv, x=rv.index, y=rv.columns, labels={"value":"Vol ann.","variable":"Actif","index":"Date"}, **range_kwargs(start_date,end_date))
    st.plotly_chart(fig_rv, use_container_width=True, theme="streamlit")
    st.caption("**Pourquoi** â€” Suivi du **risque dynamique**; observe les rÃ©gimes calmes vs turbulents; compare A vs B Ã  date donnÃ©e.")

    st.subheader("CorrÃ©lation glissante")
    rc = rets[ticker_a].rolling(roll_corr_w).corr(rets[ticker_b]).to_frame("CorrÃ©lation")
    fig_rc = px.line(rc, x=rc.index, y="CorrÃ©lation", **range_kwargs(start_date,end_date))
    st.plotly_chart(fig_rc, use_container_width=True, theme="streamlit")
    st.caption("**Usage** â€” Si la corrÃ©lation plonge durablement, diversification accrue; si elle monte vers 1, bÃ©nÃ©fices de diversification sâ€™Ã©rodent.")

    st.subheader("Î² glissant (B sur A)")
    rb = rolling_beta(rets[ticker_a], rets[ticker_b], roll_beta_w).dropna()
    # âœ… SÃ©rie â†’ DataFrame (Ã©vite ValueError Plotly)
    rb_df = rb.reset_index()
    rb_df.columns = ["Date", "Î² glissant"]
    fig_rb = px.line(rb_df, x="Date", y="Î² glissant", **range_kwargs(start_date, end_date))
    st.plotly_chart(fig_rb, use_container_width=True, theme="streamlit")
    st.caption("**InterprÃ©tation** â€” Î² dynamique utile pour **hedging tactique** : ajuste les tailles long/short en fonction du Î² courant.")

    st.subheader("Sharpe glissant (annualisÃ©)")
    sharpe_roll = (rets.rolling(roll_vol_w).mean() / rets.rolling(roll_vol_w).std(ddof=1))*np.sqrt(k)
    fig_sr = px.line(sharpe_roll, x=sharpe_roll.index, y=sharpe_roll.columns, labels={"value":"Sharpe","variable":"Actif","index":"Date"}, **range_kwargs(start_date,end_date))
    st.plotly_chart(fig_sr, use_container_width=True, theme="streamlit")
    st.caption("**Comment lâ€™utiliser** â€” Suivre la **qualitÃ©** du rendement ajustÃ© du risque dans le temps; compare A/B pour choisir lâ€™actif dominant.")

# ========== PORTFOLIO ==========
with tab_port:
    st.subheader("HypothÃ¨ses (annualisÃ©es) â€” Î¼Ì‚ & Î£Ì‚ (EWMA si cochÃ©)")
    mu_est, cov_est = ewma_mu_cov(rets, lam)
    mu_ann, cov_ann = mu_est*k, cov_est*k
    c0,c1,c2 = st.columns(3)
    c0.metric(f"Î¼Ì‚ {ticker_a}", pct(mu_ann[ticker_a]))
    c1.metric(f"Î¼Ì‚ {ticker_b}", pct(mu_ann[ticker_b]))
    c2.metric("CorrÃ©lation", f"{rets[[ticker_a,ticker_b]].corr().iloc[0,1]:.3f}")
    st.dataframe(cov_ann.style.format("{:.6f}"), use_container_width=True)
    st.caption("**Note** â€” Î¼Ì‚, Î£Ì‚ estimÃ©s sur la fenÃªtre sÃ©lectionnÃ©e ; EWMA (Î») met plus de poids au rÃ©cent (RiskMetrics).")

    rf = rf_annual
    w_min = min_var_w(cov_ann)
    w_tan = tangency_w(mu_ann, cov_ann, rf)
    vols = pd.Series(np.sqrt(np.diag(cov_ann.values)), index=cov_ann.columns)
    w_rp = (1/vols)/(1/vols).sum()
    b_ba, _, _ = beta_alpha_ols(rets[ticker_a], rets[ticker_b])
    w_beta = pd.Series({ticker_a:1.0, ticker_b:-b_ba}); w_beta = w_beta/w_beta.abs().sum()
    if long_only:
        for W in [w_min, w_tan, w_rp]:
            W[W<0]=0; s=W.sum(); 
            if s>0: W/=s

    rows=[]
    for name,w in [("Equal-Weight", pd.Series([0.5,0.5], index=[ticker_a,ticker_b])),
                   ("Min-Variance", w_min),
                   ("Tangency", w_tan),
                   ("Risk-Parity", w_rp),
                   ("Î²-neutral", w_beta)]:
        mu_p, sig_p, S = port_stats(w, mu_ann, cov_ann, rf)
        rows.append({"Portfolio":name, f"w_{ticker_a}":w[ticker_a], f"w_{ticker_b}":w[ticker_b], "Î¼ (ann.)":mu_p, "Ïƒ (ann.)":sig_p, "Sharpe":S})
    port_tbl = pd.DataFrame(rows)
    st.subheader("Poids & performances (annuelles)")
    st.dataframe(port_tbl.style.format({f"w_{ticker_a}":"{:.2%}", f"w_{ticker_b}":"{:.2%}", "Î¼ (ann.)":"{:.2%}", "Ïƒ (ann.)":"{:.2%}", "Sharpe":"{:.2f}"}), use_container_width=True)
    st.caption("**Lecture** â€” Compare les architectures de portefeuille. **Tangency** maximise Sharpe; **Min-Var** minimise le risque; **Risk-Parity** Ã©galise les contributions de risque; **Î²-neutral** vise neutralitÃ© directionnelle entre A & B.")

    # FrontiÃ¨re efficiente (2 actifs)
    w = np.linspace(0,1,201)
    mu_a, mu_b = mu_ann[ticker_a], mu_ann[ticker_b]
    var_a, var_b = cov_ann.loc[ticker_a,ticker_a], cov_ann.loc[ticker_b,ticker_b]
    cov_ab = cov_ann.loc[ticker_a,ticker_b]
    mu_pf = w*mu_a+(1-w)*mu_b
    sig_pf = np.sqrt((w**2)*var_a+((1-w)**2)*var_b+2*w*(1-w)*cov_ab)
    fig_front = px.line(x=sig_pf, y=mu_pf, labels={"x":"Ïƒ (ann.)","y":"Î¼ (ann.)"}, title="Efficient Frontier (2 actifs)")
    mu_min, sig_min, _ = port_stats(w_min, mu_ann, cov_ann, rf)
    mu_tan, sig_tan, _ = port_stats(w_tan, mu_ann, cov_ann, rf)
    fig_front.add_scatter(x=[sig_min], y=[mu_min], mode="markers", name="Min-Var")
    fig_front.add_scatter(x=[sig_tan], y=[mu_tan], mode="markers", name="Tangency")
    st.plotly_chart(fig_front, use_container_width=True, theme="streamlit")
    st.caption("**Utilisation** â€” Choisis le point sur la frontiÃ¨re selon ton budget de risque (Ïƒ) et ton objectif de rendement (Î¼).")

    # Backtest cumulÃ© pÃ©riodique
    st.subheader("Backtest cumulÃ© (base=1)")
    weights = {"Equal":pd.Series([0.5,0.5], index=[ticker_a,ticker_b]),
               "MinVar":w_min, "Tangency":w_tan, "RiskParity":w_rp, "BetaNeutral":w_beta}
    cum = pd.DataFrame(index=rets.index)
    for name,w in weights.items():
        r = (rets*w).sum(axis=1)
        cum[name] = (1+r).cumprod()
    fig_cum = px.line(cum, x=cum.index, y=cum.columns, **range_kwargs(start_date,end_date))
    st.plotly_chart(fig_cum, use_container_width=True, theme="streamlit")
    st.caption("**Attention** â€” Backtest simple, rÃ©Ã©quilibrage implicite Ã  chaque pÃ©riode; pas de coÃ»ts de transaction, pas de frictions.")

# ========== PAIR TRADING ==========
with tab_pair:
    st.subheader("Cointegration & Spread (OLS sur prix)")
    X = sm.add_constant(prices[ticker_a]); model = sm.OLS(prices[ticker_b], X).fit()
    hedge = float(model.params[1])
    spread = prices[ticker_b] - hedge*prices[ticker_a]
    z = (spread - spread.rolling(60).mean())/spread.rolling(60).std(ddof=1)
    hl = ou_half_life(spread)

    kpi = st.columns(4)
    kpi[0].metric("Hedge ratio b", f"{hedge:.3f}")
    try:
        kpi[1].metric("p-val cointÃ©gration", f"{coint(prices[ticker_a], prices[ticker_b])[1]:.3f}")
    except Exception:
        kpi[1].metric("p-val cointÃ©gration", "â€”")
    kpi[2].metric("Half-life OU", f"{hl:.1f}" if not pd.isna(hl) else "â€”")
    kpi[3].metric("Z-score actuel", f"{z.iloc[-1]:.2f}")

    # âœ… SÃ©rie â†’ DataFrame avant plotting
    sp_df = spread.reset_index()
    sp_df.columns = ["Date", "Spread"]
    fig_sp = px.line(sp_df, x="Date", y="Spread", **range_kwargs(start_date,end_date))
    st.plotly_chart(fig_sp, use_container_width=True, theme="streamlit")
    st.caption("**IdÃ©e** â€” Spread = B âˆ’ bÂ·A; si cointÃ©grÃ©s, le spread est mean-reverting. **Trades** (Ã©ducatif) : |Z|>2 â‡’ contrarien; sortie vers Zâ‰ˆ0.")

    st.subheader("Z-score du spread (fenÃªtre 60)")
    Z = z.rename("Z").dropna().reset_index()
    Z.columns = ["Date", "Z"]
    fig_z = px.line(Z, x="Date", y="Z", **range_kwargs(start_date,end_date))
    fig_z.add_hline(y=0, line_dash="dot"); fig_z.add_hline(y=2, line_dash="dash", annotation_text="+2")
    fig_z.add_hline(y=-2, line_dash="dash", annotation_text="-2")
    st.plotly_chart(fig_z, use_container_width=True, theme="streamlit")
    st.caption("**Lecture** â€” Zones Â±2Ïƒ = anomalies; surveille la durÃ©e passÃ©e en extrÃªme et la vitesse de reversion (half-life).")

# ========== ADVANCED RISK ==========
with tab_risk:
    st.subheader("EVT (POTâ€“GPD) sur pertes (VaR/ES extrÃªmes)")
    cols = st.columns(2)
    for i,(label, r) in enumerate([(ticker_a, -rets[ticker_a]), (ticker_b, -rets[ticker_b])]):
        r = r.dropna()
        if len(r)<200:
            cols[i].warning(f"{label}: sÃ©rie trop courte pour EVT.")
            continue
        u = np.quantile(r, 0.95)         # seuil 95% des pertes
        excess = r[r>u] - u
        try:
            c, loc, scale = genpareto.fit(excess.values, floc=0)
            p_tail = (excess.size)/len(r) # prob. dâ€™excÃ¨s
            p = 0.99
            if c != 0:
                var_p = u + scale/c * (((1-p)/p_tail)**(-c) - 1)
            else:
                var_p = u + scale*np.log(p_tail/(1-p))
            es_p = (var_p + (scale - c*u)/(1-c)) if c<1 else np.nan
            cols[i].metric(f"EVT VaR99 {label}", pct(var_p))
            cols[i].metric(f"EVT ES99  {label}", pct(es_p))
        except Exception:
            cols[i].warning(f"{label}: Ã©chec ajustement GPD.")
    st.caption("**Pourquoi** â€” EVT modÃ©lise les **queues** extrÃªmes mieux que la normale; pertinent pour le **risk-management** en stress.")

    st.subheader("CoVaR (RÃ©gression quantile, q=5%)")
    try:
        qa = rets[ticker_a].quantile(0.05)
        qb = rets[ticker_b].quantile(0.05)
        qmod = QuantReg(rets[ticker_b], sm.add_constant(rets[ticker_a])).fit(q=0.05)
        covar_b_a = float(qmod.predict([1, qa])[0])
        qmod_med = QuantReg(rets[ticker_b], sm.add_constant(rets[ticker_a])).fit(q=0.5)
        d_covar_b_a = covar_b_a - float(qmod_med.predict([1, rets[ticker_a].median()])[0])

        qmod2 = QuantReg(rets[ticker_a], sm.add_constant(rets[ticker_b])).fit(q=0.05)
        covar_a_b = float(qmod2.predict([1, qb])[0])
        qmod2_med = QuantReg(rets[ticker_a], sm.add_constant(rets[ticker_b])).fit(q=0.5)
        d_covar_a_b = covar_a_b - float(qmod2_med.predict([1, rets[ticker_b].median()])[0])

        c = st.columns(4)
        c[0].metric(f"CoVaR5% {ticker_b}|{ticker_a}", pct(-covar_b_a))
        c[1].metric("Î”CoVaR B|A", pct(-d_covar_b_a))
        c[2].metric(f"CoVaR5% {ticker_a}|{ticker_b}", pct(-covar_a_b))
        c[3].metric("Î”CoVaR A|B", pct(-d_covar_a_b))
    except Exception:
        st.warning("Quantile Regression Ã©chouÃ©e (Ã©chantillon trop court ou colinÃ©aritÃ©).")
    st.caption("**InterprÃ©tation** â€” CoVaR mesure le **risque conditionnel**: pertes de B **donnÃ©es** une dÃ©tresse dâ€™A (quantile bas).")

    st.subheader("RÃ©gimes markoviens (variance switching) â€” 2 Ã©tats")
    if enable_heavy:
        try:
            y = (rets.mean(axis=1)).dropna()
            mod = MarkovRegression(y, k_regimes=2, trend='c', switching_variance=True)
            res = mod.fit(disp=False)
            pr = res.smoothed_marginal_probabilities[1]  # rÃ©gime de variance Ã©levÃ©e
            pr_df = pr.reset_index()
            pr_df.columns = ["Date", "ProbHighVar"]
            fig_m = px.line(pr_df, x="Date", y="ProbHighVar", **range_kwargs(start_date,end_date))
            st.plotly_chart(fig_m, use_container_width=True, theme="streamlit")
            st.caption("**Usage** â€” DÃ©tecte rÃ©gimes calmes vs turbulents; utile pour **switching** de levier/exposition.")
        except Exception:
            st.info("RÃ©gimes markoviens indisponibles (optimisation). Essaie une pÃ©riode plus longue.")
    else:
        st.info("Active â€œModÃ¨les lourdsâ€ pour estimer les rÃ©gimes markoviens.")

# ========== DIAGNOSTICS ==========
with tab_diag:
    ra, rb = rets[ticker_a].dropna(), rets[ticker_b].dropna()

    def adf_p(x): 
        try: return float(adfuller(x)[1])
        except: return np.nan
    def ljung_p(x, l=10):
        try: return float(acorr_ljungbox(x, lags=[l], return_df=True)["lb_pvalue"].iloc[0])
        except: return np.nan
    def arch_p(x, l=10):
        try: return float(het_arch(x, nlags=l)[1])
        except: return np.nan
    def jb_p(x):
        try: return float(jarque_bera(x)[1])
        except: return np.nan
    def granger_min(x_from, y_to, L=5):
        try:
            df = pd.concat([y_to, x_from], axis=1).dropna()
            if len(df)<50: return np.nan
            res = grangercausalitytests(df, maxlag=L, verbose=False)
            p = [res[i][0]["ssr_ftest"][1] for i in range(1, L+1)]
            return float(np.nanmin(p))
        except: return np.nan

    grid = pd.DataFrame({
        "Test": ["ADF", "Ljung-Box(10)", "ARCH(10)", "Jarque-Bera", "Granger Aâ†’B", "Granger Bâ†’A"],
        ticker_a: [adf_p(ra), ljung_p(ra), arch_p(ra), jb_p(ra), granger_min(ra, rb), np.nan],
        ticker_b: [adf_p(rb), ljung_p(rb), arch_p(rb), jb_p(rb), np.nan, granger_min(rb, ra)],
    })
    st.subheader("Tests statistiques (p-values)")
    st.dataframe(grid, use_container_width=True)
    st.caption("**Rappels** â€” ADF: stationnaritÃ©; Ljung-Box: autocorr; ARCH: hÃ©tÃ©roscÃ©dasticitÃ©; JB: normalitÃ©; Granger: causalitÃ© temporelle. Seuils usuels p<0,05.")

    st.subheader("Histogrammes des rendements")
    hc = st.columns(2)
    hc[0].plotly_chart(px.histogram(ra, nbins=60, labels={"value":f"Rendements {ticker_a}"}), use_container_width=True, theme="streamlit")
    hc[1].plotly_chart(px.histogram(rb, nbins=60, labels={"value":f"Rendements {ticker_b}"}), use_container_width=True, theme="streamlit")
    st.caption("**Astuce** â€” Queue lourde et asymÃ©trie visibles Ã  lâ€™Å“il; valide/infirme lâ€™hypothÃ¨se gaussienne.")

# ========== DÃ‰PENDANCES DE QUEUE & LAGS ==========
with tab_dep:
    st.subheader("CorrÃ©lations de queue (conditionnelles)")
    lc = tail_corr(rets[ticker_a], rets[ticker_b], q=0.05, upper=False)
    uc = tail_corr(rets[ticker_a], rets[ticker_b], q=0.05, upper=True)
    cc = st.columns(2)
    cc[0].metric("CorrÃ©l. queue basse (5%)", f"{lc:.3f}" if not pd.isna(lc) else "â€”")
    cc[1].metric("CorrÃ©l. queue haute (95%)", f"{uc:.3f}" if not pd.isna(uc) else "â€”")
    st.caption("**IdÃ©e** â€” La dÃ©pendance en queue **basse** (co-krash) est cruciale pour la gestion des risques; la queue **haute** aide Ã  comprendre les rallies synchrones.")

    st.subheader("Cross-correlation lead/lag")
    xct = xcorr_at_lags(rets[ticker_a], rets[ticker_b], lags=lags_xcorr)
    st.dataframe(xct.style.format({"corr Aâ†’B":"{:.3f}", "corr Bâ†’A":"{:.3f}"}), use_container_width=True)
    st.caption("**Lecture** â€” Corr Aâ†’B au lag k = corr(B_t, A_{t-k}). Si Ã©levÃ©e, A **prÃ©cÃ¨de** B (signal prÃ©dictif potentiel). Attention aux faux positifs.")

# ========== DATA ==========
with tab_data:
    st.subheader("ParamÃ¨tres")
    st.write({
        "Tickers": [ticker_a, ticker_b],
        "FrÃ©quence": freq_key,
        "Plage": {"start": str(start_date), "end": str(end_date)},
        "Log returns": log_ret, "RF (ann.)": rf_annual,
        "FenÃªtres": {"vol": roll_vol_w, "corr": roll_corr_w, "beta": roll_beta_w},
        "EWMA Î»": lam, "Long-only": long_only, "ModÃ¨les lourds": enable_heavy,
        "Buffer jours (fetch)": pre_buffer_days,
        "Couverture observÃ©e": f"{coverage:.0%}"
    })

    st.subheader("Prix (filtrÃ©s sur la plage)")
    st.dataframe(prices, use_container_width=True)
    st.download_button("ðŸ“¥ PRIX (CSV)", data=prices.to_csv().encode("utf-8"), file_name="prices.csv")

    st.subheader("Rendements (selon frÃ©quence)")
    st.dataframe(rets, use_container_width=True)
    st.download_button("ðŸ“¥ RENDEMENTS (CSV)", data=rets.to_csv().encode("utf-8"), file_name="returns.csv")

st.caption("âš ï¸ Outil d'analyse avancÃ©e Ã  visÃ©e pÃ©dagogique. Non destinÃ© au conseil en investissement.")
